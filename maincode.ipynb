{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Imports necessary libraries for loading documents, managing environment variables, and building the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader =  DirectoryLoader(data,\n",
    "                    glob = \"*.pdf\",\n",
    "                    loader_cls= PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Defines a function to scan the specified directory and extract text from all PDF files found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")\n",
    "\n",
    "# Calls the load_pdf function to process and extract raw text data from all PDFs located in the \"data/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "# Splits the extracted text into smaller, overlapping chunks (500 chars) to prepare them for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))\n",
    "\n",
    "# Executes the chunking process and prints the total count of resulting text segments to verify the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "\n",
    "def download_hugging_face_embedddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MinilM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# Initializes and downloads the specific Hugging Face model used to convert text chunks into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embedddings()\n",
    "\n",
    "# Calls the function to download and initialize the embedding model, storing it in the 'embeddings' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key= os.getenv('PINECONE_API_KEY')\n",
    "os.environ['PINECONE_API_KEY'] \n",
    "index_name = \"test-chatbot\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks],embeddings,  index_name=index_name\n",
    ") #run this line only once initially\n",
    "\n",
    "# Retrieves the API key and uploads the vectorized text chunks into the specified Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# Connects to an already populated Pinecone index to enable searching without re-uploading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template =\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return helpful answer below and anothing else\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "#Defines the prompt structure that guides the LLM to answer questions based strictly on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\" , \"question\"])\n",
    "chain_type_kwargs = {\"prompt\" : PROMPT}\n",
    "\n",
    "# Initializes the custom prompt object and packages it into a dictionary to configure the retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={\n",
    "                      'max_new_tokens':512,\n",
    "                         'temperature':0.8})\n",
    "\n",
    "# Loads the local quantized Llama 2 model and configures generation parameters like token limit and creativity (temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k':2}), \n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "# Constructs the main Question-Answering chain that connects the LLM, the retriever (Pinecone), and the prompt to answer queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(f\"Input Prompt:\")\n",
    "    result=qa.invoke({\"query\" : user_input})\n",
    "    print(\"Response :\" , result[\"result\"])\n",
    "\n",
    "# Starts an interactive loop to continuously accept user input, run the QA chain, and print the answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
